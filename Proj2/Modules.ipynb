{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modules.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DC5U0DCJBLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import Tensor, FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOINdC3TUmiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Base Class Module\n",
        "\n",
        "class Module(object):\n",
        "  '''\n",
        "  Base class for other neural network modules to inherit from.\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    self._author = 'Ahmed Ben Haj Yahia, Amir Ghali, Mahmoud Sellami.'\n",
        "  \n",
        "  def forward(self, *input):\n",
        "    '''\n",
        "    Should get for input, and returns, a tensor or a tuple of tensors.\n",
        "    '''\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(self, *gradwrtoutput):\n",
        "    '''\n",
        "    Should get as input a tensor or a tuple of tensors containing the gradient of the loss\n",
        "    with respect to the module’s output, accumulate the gradient wrt the parameters, and return a\n",
        "    tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
        "    '''\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def param(self):\n",
        "    '''\n",
        "    Should return a list of pairs, each composed of a parameter tensor, and a gradient tensor\n",
        "    of same size. This list should be empty for parameterless modules (e.g. ReLU).\n",
        "    '''\n",
        "    return []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f82szlyUmlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Module Class: Linear\n",
        "\n",
        "class Linear(Module):\n",
        "  '''\n",
        "  Fully Connected Layer defined by its input and output dimensions\n",
        "  '''\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, mean_value = 0, std_value = 1):\n",
        "    super().__init__()\n",
        "    self.x = 0\n",
        "    self.w = Tensor(output_dim, input_dim).normal_(mean = mean_value, std = std_value)\n",
        "    self.b = Tensor(output_dim).normal_(mean = mean_value, std = std_value)\n",
        "    self.dl_w = Tensor(self.w.size())\n",
        "    self.dl_b = Tensor(self.b.size())\n",
        "  \n",
        "  def forward(self, input):\n",
        "    self.x = input\n",
        "    return self.w.mv(self.x) + self.b\n",
        "\n",
        "  def backward(self, gradwrtoutput):\n",
        "    self.dl_w.add_(gradwrtoutput.view(-1,1).mm(self.x.view(1,-1)))\n",
        "    self.dl_b.add_(gradwrtoutput)\n",
        "    return self.w.t().mv(gradwrtoutput)\n",
        "\n",
        "  def param(self):\n",
        "    return [(self.w, self.dl_w),(self.b, self.dl_b)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSfdsrTC3dpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Module Class: ReLU\n",
        "\n",
        "class ReLU(Module):\n",
        "  '''\n",
        "  Activation Function: x → max(0, x)\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.x = 0\n",
        "    \n",
        "  def forward(self, input):\n",
        "    self.x = input\n",
        "    return self.x.clamp(min=0)\n",
        " \n",
        "  def backward(self, gradwrtoutput):\n",
        "    dl_relu = self.x.sign().clamp(min=0)\n",
        "    return dl_relu * gradwrtoutput\n",
        "\n",
        "  def param(self):\n",
        "    return [(None,None)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEwe1QgzUmoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Module Class: Tanh\n",
        "\n",
        "class Tanh(Module):\n",
        "  '''\n",
        "  Activation Function: x → [2/(1 + e−2x)] - 1\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.x = 0\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.x = input\n",
        "    return (self.x.exp() - (-self.x).exp())/(self.x.exp()+(-self.x).exp())\n",
        "  \n",
        "  def backward(self, gradwrtoutput):\n",
        "    dl_tanh =  self.x.clone().fill_(1) - self.forward(self.x).pow(2)\n",
        "    return dl_tanh * gradwrtoutput\n",
        "\n",
        "  def param(self):\n",
        "    return [(None,None)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxInMyULC6wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Module Class: Sequential\n",
        "\n",
        "class Sequential(Module):\n",
        "  '''\n",
        "  Combines several linear and non-linear modules in a Sequential structure (The Multi-Layer Perceptron)\n",
        "  '''\n",
        "\n",
        "  def __init__(self, *args):\n",
        "    super().__init__()\n",
        "    self.modules = []\n",
        "\n",
        "    for module in args:\n",
        "      if(isinstance(module,Module)):\n",
        "        self.add_module(module)\n",
        "      else:\n",
        "        raise ArgumentError(\"Only modules can be passed as parameters to Sequential Module\")\n",
        "  \n",
        "  def add_module(self, module):\n",
        "    self.modules.append(module)\n",
        "  \n",
        "\n",
        "  def forward(self, input):\n",
        "    out = input\n",
        "    for module in self.modules:\n",
        "      out = module.forward(out)\n",
        "    return out\n",
        "\n",
        "  def backward(self, grdwrtoutput):\n",
        "    reversed_list = self.modules[::-1]\n",
        "    grad = grdwrtoutput\n",
        "    for module in reversed_list:\n",
        "      grad = module.backward(grad)\n",
        "\n",
        "  def param(self):\n",
        "    parameters = []\n",
        "    for module in self.modules:\n",
        "      parameters.append(module.param())\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir0oWp6EzbPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Optimizer Class: SGD\n",
        "\n",
        "class SGD():\n",
        "  '''\n",
        "  Stochastic Gradient Descent Optimizer\n",
        "\n",
        "  zero_grad() : Clears all the parameter's gradients\n",
        "  step() : Does one optimization step (Updating the parameters)\n",
        "  '''\n",
        "\n",
        "  def __init__(self, params, lr):\n",
        "    if lr < 0.0 :\n",
        "      raise ValueError(\"Invalid learning rate: {} -- [Should be positive]\".format(lr))\n",
        "    \n",
        "    self.params = params\n",
        "    self.lr = lr\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for module in self.params:\n",
        "      for pair in module:\n",
        "        param, dl_param = pair\n",
        "        if (param is None) or (dl_param is None):\n",
        "          continue\n",
        "        else:\n",
        "          dl_param.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    for module in self.params:\n",
        "      for pair in module:\n",
        "        param, dl_param = pair\n",
        "        if (param is None) or (dl_param is None):\n",
        "          continue\n",
        "        else:\n",
        "          param.add_(-self.lr * dl_param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8C11FeDUmrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loss Function: MSE\n",
        "\n",
        "def MSELoss(target, pred):\n",
        "  '''\n",
        "  Outputs the MSE Loss as a float value\n",
        "  '''\n",
        "  return (pred - target.float()).pow(2).sum()\n",
        "\n",
        "\n",
        "def dl_MSELoss(target, pred):\n",
        "  '''\n",
        "  Outputs gradient's Loss as a Tensor\n",
        "  '''\n",
        "  return 2*(pred - target.float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4BmQpM8bK1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}